{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Workflows in NOMAD","text":"<p>Workflows are an important aspect of data management as they enable a systematic organization of the tasks performed during any Materials Science research project. We refer to a workflow as a series of experiments or simulations composed of inputs, outputs, and tasks performed either in serial or in parallel. Each entry in NOMAD has a workflow section, describing how the (meta)data within the entry was generated. Additionally, an \"overarching\" workflow can be generated within its own entry, to define connections between multiple entries (and subworkflows) via references to the corresponding entries and sections.</p> <p>The general schema for a workflow in NOMAD (found under <code>nomad.datamodel.metainfo.workflow</code>) can be represented with the following graph:</p> <p> </p> <p>The NOMAD workflow (blue section in the above image) is section of an entry in the NOMAD Archive. The workflow subsection<code>Task</code> contains information about each of the tasks performed within the workflow. The workflow subsection <code>TaskReference</code> allows to reference other tasks or workflows. Finally, the workflow subsection <code>Link</code> allows to link between tasks and sections within the NOMAD Archive.</p> <p>This documentation will show you:  </p> <ul> <li>A simple tutorial to understand the managing and definition of custom workflows in NOMAD.</li> <li>...</li> </ul>"},{"location":"tutorial/","title":"Tutorial","text":""},{"location":"tutorial/#introduction","title":"Introduction","text":"<p>We will use a ficticious example of a simulation workflow, where the files and folder structure is:</p> <p><pre><code>.\n\u251c\u2500\u2500 pressure1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p1.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p1.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u2514\u2500\u2500 pressure2\n \u00a0\u00a0 \u251c\u2500\u2500 temperature1\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t1.hdf5\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n \u00a0\u00a0 \u251c\u2500\u2500 temperature2\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t2.hdf5\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n \u00a0\u00a0 \u251c\u2500\u2500 dft_p2.xml\n \u00a0\u00a0 \u251c\u2500\u2500 tb_p2.wout\n \u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n</code></pre> Each of the mainfiles represent an electronic-structure calculation (either DFT, TB, or DMFT) which in turn is then parsed into a singular entry in NOMAD. When dragged into the NOMAD Upload page, these files should generate 8 entries in total. This folder structure presents a typical workflow calculation which can be represented as a provenance graph: <pre><code>graph LR;\n    A((Inputs)) --&gt; B1[DFT];\n    A((Inputs)) --&gt; B2[DFT];\n    subgraph pressure P&lt;sub&gt;2&lt;/sub&gt;\n    B2[DFT] --&gt; C2[TB];\n    C2[TB] --&gt; D21[DMFT at T&lt;sub&gt;1&lt;/sub&gt;];\n    C2[TB] --&gt; D22[DMFT at T&lt;sub&gt;2&lt;/sub&gt;];\n    end\n    D21[DMFT at T&lt;sub&gt;1&lt;/sub&gt;] --&gt; E21([Output calculation P&lt;sub&gt;2&lt;/sub&gt;, T&lt;sub&gt;1&lt;/sub&gt;])\n    D22[DMFT at T&lt;sub&gt;2&lt;/sub&gt;] --&gt; E22([Output calculation P&lt;sub&gt;2&lt;/sub&gt;, T&lt;sub&gt;2&lt;/sub&gt;])\n    subgraph pressure P&lt;sub&gt;1&lt;/sub&gt;\n    B1[DFT] --&gt; C1[TB];\n    C1[TB] --&gt; D11[DMFT at T&lt;sub&gt;1&lt;/sub&gt;];\n    C1[TB] --&gt; D12[DMFT at T&lt;sub&gt;2&lt;/sub&gt;];\n    end\n    D11[DMFT at T&lt;sub&gt;1&lt;/sub&gt;] --&gt; E11([Output calculation P&lt;sub&gt;1&lt;/sub&gt;, T&lt;sub&gt;1&lt;/sub&gt;])\n    D12[DMFT at T&lt;sub&gt;2&lt;/sub&gt;] --&gt; E12([Output calculation P&lt;sub&gt;1&lt;/sub&gt;, T&lt;sub&gt;2&lt;/sub&gt;])</code></pre> Here, \"Input\" refers to the all input information given to perform the calculation (e.g., atom positions, model parameters, experimental initial conditions, etc.). \"DFT\", \"TB\" and \"DMFT\" refer to individual tasks of the workflow, which each correspond to a SinglePoint entry in NOMAD. \"Output calculation\" refers to the output data of each of the final DMFT tasks.</p> <p>The goal of this tutorial is to set up the following workflows:</p> <ol> <li>A <code>SinglePoint</code> workflow for one of the calculations (e.g., the DFT one) in the <code>pressure1</code> subfolder.</li> <li>An overarching workflow entry for each pressure Pi=1,2, grouping all <code>SinglePoint</code> \"DFT\", \"TB\", \"DMFT at T1\", and \"DMFT at T2\" tasks.</li> <li>A top level workflow entry, grouping together all pressure calculations.</li> </ol>"},{"location":"tutorial/#starting-example-singlepoint-workflow","title":"Starting example: SinglePoint workflow","text":"<p>NOMAD is able to recognize certain workflows in an automatic way, such as the <code>SinglePoint</code> case mentioned above. However, to showcase how to the use workflows in NOMAD, we will \"manually\" construct the SinglePoint workflow, represented by the following provenance graph: <pre><code>graph LR;\n    subgraph SinglePoint\n    A((Input structure)) --&gt; B[DFT];\n    B[DFT] --&gt; C([Output calculation]);\n    end</code></pre></p> <p>To define a workflow manually in NOMAD, we must add a <code>YAML</code> file to the upload folder that contains the relevant input, output, and task information. This file should be named <code>&lt;filename&gt;.archive.yaml</code>1. In this case, we include the file <code>single_point.archive.yaml</code> with the following content:</p> <pre><code>workflow2:\nname: SinglePoint\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\noutputs:\n- name: Output calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\ntasks:\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\nname: DFT at Pressure P1\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\noutputs:\n- name: Output calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n</code></pre> <p>We can note several things about the content of this file:</p> <ol> <li><code>name</code> keys are optional.</li> <li>The root path of the upload can be referenced with <code>../upload/archive/mainfile/</code>. Starting from there, the original directory tree structure of the upload is maintained.</li> <li><code>inputs</code> reference the section containing inputs of the whole workflow. In this case this is the section <code>run[0].system[-1]</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>outputs</code> reference the section containing outputs of the whole workflow. In this case this is the section <code>run[0].calculation[-1]</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>tasks</code> reference the section containing tasks of each step in the workflow. These must also contain <code>inputs</code> and <code>outputs</code> properly referencing the corresponding sections; this will then link inputs/outputs/tasks in the NOMAD Archive. In this case this is a <code>TaskReference</code> to the section <code>workflow2</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>section</code> reference to the uploaded mainfile specific section. The left side of the <code>#</code> symbol contains the path to the mainfile, while the right contains the path to the section.</li> </ol> <p>This will produce an extra entry with the following Overview content:</p> <p> </p> <p>Note that we are referencing sections which are lists. Thus, in each case we have to be careful to reference the correct section for inputs and outputs (example: a <code>GeometryOptimization</code> workflow calculation will have the \"Input structure\" as <code>run[0].system[0]</code>, while the \"Output calculation\" would also contain <code>run[0].system[-1]</code>, and all intermediate steps must input/output the corresponding section system).</p> <p>We can extend the workflow meta-information by adding the metholodogical input parameters. These are stored in NOMAD in the section path <code>run[0].method[-1]</code>. The new <code>single_point.archive.yaml</code> will be:</p> <pre><code>workflow2:\nname: SinglePoint\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n- name: Input methodology parameters\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\noutputs:\n- name: Output calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\ntasks:\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\nname: DFT at Pressure P1\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n- name: Input methodology parameters\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\noutputs:\n- name: Output calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n</code></pre> <p>which in turn produces a similar workflow than before, but with an extra input node:</p> <p> </p>"},{"location":"tutorial/#pressure-workflows","title":"Pressure workflows","text":"<p>Now that we know the basics of the workflow <code>YAML</code> schema, let's try to define an overarching workflow for each of the pressures. For this section, we will show the case of P1; the extension for P2 is then a matter of changing names and paths in the <code>YAML</code> files. For simplicity, we will skip referencing to methodologies.</p> <p>Thus, the <code>inputs</code> can be defined as: <pre><code>workflow2:\nname: DFT+TB+DMFT at P1\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n</code></pre> and there are two <code>outputs</code>, one for each of the DMFT calculations at distinct temperatures: <pre><code>  outputs:\n- name: Output DMFT at P1, T1 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P1, T2 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n</code></pre> Now, <code>tasks</code> are defined for each of the methodologies performed (each corresponding to an underlying SinglePoint workflow). To define a valid workflow, each task must contain an input that corresponds to one of the outputs of the previous task. Moreover, the first task should take as input the overall input of the workflow, and the final task should also have as an output the overall workflow output. Then: <pre><code>  tasks:\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\nname: DFT at P1\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\noutputs:\n- name: Output DFT at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1/tb_p1.wout#/workflow2'\nname: TB at P1\ninputs:\n- name: Input DFT at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\noutputs:\n- name: Output TB at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/workflow2'\nname: DMFT at P1 and T1\ninputs:\n- name: Input TB at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\noutputs:\n- name: Output DMFT at P1, T1 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/workflow2'\nname: DMFT at P1 and T2\ninputs:\n- name: Input TB at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\noutputs:\n- name: Output DMFT at P1, T2 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n</code></pre> We can note here:</p> <ul> <li>The <code>inputs</code> for each subsequent step are the <code>outputs</code> of the previous step.</li> <li>The final two <code>outputs</code> coincide with the <code>workflow2</code> <code>outputs</code>.</li> </ul> <p>This workflow (<code>pressure1.archive.yaml</code>) file will then produce an entry with the following Overview page:</p> <p> </p> <p>Similarly, for P2 we can upload a new <code>pressure2.archive.yaml</code> file with the same content, except when substituting 'pressure1' and 'p1' by their counterparts. This will produce a similar graph than the one showed before but for 'P2'.</p>"},{"location":"tutorial/#the-top-level-workflow","title":"The top-level workflow","text":"<p>After adding the workflow YAML files, our upload folder directory now looks like: <pre><code>.\n\u251c\u2500\u2500 pressure1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p1.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p1.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u251c\u2500\u2500 pressure1.archive.yaml\n\u251c\u2500\u2500 pressure2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p2.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p2.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u251c\u2500\u2500 pressure2.archive.yaml\n\u2514\u2500\u2500 single_point.archive.yaml\n</code></pre> In order to define the general workflow that groups all pressure calculations, we can reference directly the previous <code>pressureX.archive.yaml</code> files as tasks. Still, <code>inputs</code> and <code>outputs</code> must be referenced to their corresponding mainfile and section paths.</p> <p>We then create a new <code>fullworkflow.archive.yaml</code> file with the <code>inputs</code>: <pre><code>workflow2:\nname: Full calculation at different pressures for SrVO3\ninputs:\n- name: Input structure at P1\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n- name: Input structure at P2\nsection: '../upload/archive/mainfile/pressure2/dft_p2.xml#/run/0/system/-1'\n</code></pre> And <code>outputs</code>: <pre><code>  outputs:\n- name: Output DMFT at P1, T1 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P1, T2 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P2, T1 calculation\nsection: '../upload/archive/mainfile/pressure2/temperature1/dmft_p2_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P2, T2 calculation\nsection: '../upload/archive/mainfile/pressure2/temperature2/dmft_p2_t2.hdf5#/run/0/calculation/-1'\n</code></pre> Finally, <code>tasks</code> references the previous YAML schemas as follows: <pre><code>  tasks:\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1.archive.yaml#/workflow2'\nname: DFT+TB+DMFT at P1\ninputs:\n- name: Input structure at P1\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\noutputs:\n- name: Output DMFT at P1, T1 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P1, T2 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure2.archive.yaml#/workflow2'\nname: DFT+TB+DMFT at P2\ninputs:\n- name: Input structure at P2\nsection: '../upload/archive/mainfile/pressure2/dft_p2.xml#/run/0/system/-1'\noutputs:\n- name: Output DMFT at P2, T1 calculation\nsection: '../upload/archive/mainfile/pressure2/temperature1/dmft_p2_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P2, T2 calculation\nsection: '../upload/archive/mainfile/pressure2/temperature2/dmft_p2_t2.hdf5#/run/0/calculation/-1'\n</code></pre></p> <p>This will produce the following entry and its Overview page:</p> <p> </p>"},{"location":"tutorial/#automatic-workflows","title":"Automatic workflows","text":"<p>There are some cases where the NOMAD infrastructure is able to recognize certain workflows automatically when processing the uploaded files. The simplest example is any <code>SinglePoint</code> calculation, as explained above. Other examples include <code>GeometryOptimization</code>, <code>Phonons</code>, <code>GW</code>, and <code>MolecularDynamics</code>. Automated workflow detection may require your folder structure to fulfill certain conditions.</p> <p>Here are some general guidelines for preparing your upload folder in order to make it easier for the automatic workflow recognition to work:</p> <ul> <li>Always organize your files in an upwards-downwards structure, i.e., the initial tasks should be upper in the directory tree, while the later tasks lower on it.</li> <li>Avoid having to go up and down between folders if some properties are derived between these files. These situations are very complicated to predict for the current NOMAD infrastructure.</li> <li>Avoid duplication of files in subfolders. If initially you do a calculation A from which a later calculation B is derived and you want to store B in a subfolder, there is no need to copy the A files inside the subfolder B.</li> </ul> <p>The folder structure used throughout this Tutorial is a good example of a clean upload which is friendly and easy to work with when defining NOMAD workflows.</p> <ol> <li> <p><code>&lt;filename&gt;</code> can be any custom name defined by the user, but the file must keep the extension <code>.archive.yaml</code> at the end.\u00a0\u21a9</p> </li> </ol>"},{"location":"explanations/whyworkflows/","title":"Why workflows?","text":""},{"location":"howtos/same_mainfile/","title":"How-to: when the mainfile for different entries is the same","text":""},{"location":"references/refs/","title":"List of references","text":""}]}